"""
Meeting summarization and action item extraction
Uses free local models (BART, T5, or Ollama)
CPU-optimized with batching and caching
"""
from transformers import pipeline
import re
from datetime import datetime
import logging
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Model configuration
SUMMARIZER_MODEL = os.getenv('SUMMARIZER_MODEL', 'facebook/bart-large-cnn')
# Alternatives: 't5-small', 't5-base', 'google/pegasus-xsum'

# Global model cache
_SUMMARIZER = None

def get_summarizer():
    """Load and cache summarization model"""
    global _SUMMARIZER
    
    if _SUMMARIZER is None:
        logger.info(f"Loading summarization model: {SUMMARIZER_MODEL}")
        _SUMMARIZER = pipeline(
            "summarization",
            model=SUMMARIZER_MODEL,
            device=-1,  # CPU only
            framework="pt"
        )
        logger.info("Summarization model loaded")
    
    return _SUMMARIZER

def generate_summary_and_action_items(transcript, custom_prompts=None):
    """
    Generate comprehensive summary with action items
    
    Args:
        transcript: List of segments with speaker, timestamp, text
        custom_prompts: Optional dict with custom section prompts
    
    Returns:
        Formatted markdown summary with sections:
        - TL;DR
        - Key Points
        - Decisions Made
        - Action Items (with assignees and timestamps)
        - Time-Coded References
    """
    logger.info("Generating meeting summary and action items")
    
    # Concatenate all text with speaker context
    full_text = "\n".join([
        f"[{seg['start']}] {seg['speaker']}: {seg['text']}"
        for seg in transcript
    ])
    
    # Generate AI summary
    summarizer = get_summarizer()
    summaries = []
    
    # Split into chunks for processing (model has token limits)
    chunks = split_text_into_chunks(full_text, max_tokens=1024)
    
    for chunk in chunks:
        if len(chunk.split()) > 50:  # Only summarize substantial chunks
            try:
                summary = summarizer(
                    chunk,
                    max_length=150,
                    min_length=50,
                    do_sample=False,
                    truncation=True
                )
                summaries.append(summary[0]['summary_text'])
            except Exception as e:
                logger.error(f"Summarization error: {str(e)}")
                summaries.append("Error generating summary for this section.")
    
    # Extract structured information
    action_items = extract_action_items(transcript)
    key_points = extract_key_points(transcript)
    decisions = extract_decisions(transcript)
    questions = extract_questions(transcript)
    topics = extract_topics(transcript)
    
    # Format output as markdown
    output = f"""# Meeting Summary

## üìã TL;DR
{' '.join(summaries) if summaries else "No summary available."}

## üéØ Key Points
{format_key_points(key_points)}

## üó≥Ô∏è Decisions Made
{format_decisions(decisions)}

## ‚úÖ Action Items
{format_action_items(action_items)}

## ‚ùì Open Questions
{format_questions(questions)}

## üè∑Ô∏è Topics Discussed
{format_topics(topics)}

## ‚è±Ô∏è Time-Coded References
{generate_time_references(transcript)}

## üìä Meeting Statistics
- **Total Duration:** {calculate_duration(transcript)}
- **Number of Speakers:** {count_unique_speakers(transcript)}
- **Total Segments:** {len(transcript)}
- **Action Items:** {len(action_items)}
- **Decisions:** {len(decisions)}

---
*Generated by MeetingInsight AI on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    
    logger.info("Summary generation complete")
    return output

def split_text_into_chunks(text, max_tokens=1024):
    """Split text into chunks suitable for model processing"""
    words = text.split()
    chunks = []
    current_chunk = []
    
    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= max_tokens:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def extract_action_items(transcript):
    """
    Extract action items using keyword patterns and NLP
    Returns list with assignee, task, timestamp
    """
    action_keywords = [
        r"(?:will|should|need to|must|have to|going to)\s+(\w+\s+){0,5}(do|complete|finish|send|create|update|review|prepare|schedule|organize|contact|follow up|write|design|implement|test|deploy|fix|investigate|research|call|email|meet)",
        r"action item[s]?:?\s+(.+)",
        r"TODO:?\s+(.+)",
        r"task:?\s+(.+)",
        r"assign(?:ed)?\s+to\s+([A-Z][a-z]+)",
        r"([A-Z][a-z]+)\s+(?:will|should)\s+(.+)",
        r"follow up (?:on|with)\s+(.+)",
        r"next steps?:?\s+(.+)"
    ]
    
    action_items = []
    
    for segment in transcript:
        text = segment['text']
        speaker = segment['speaker']
        timestamp = segment['start']
        
        # Check each pattern
        for pattern in action_keywords:
            if re.search(pattern, text, re.IGNORECASE):
                assignee = extract_assignee(text, speaker)
                priority = extract_priority(text)
                deadline = extract_deadline(text)
                
                action_items.append({
                    'text': text,
                    'speaker': speaker,
                    'timestamp': timestamp,
                    'assigned_to': assignee,
                    'priority': priority,
                    'deadline': deadline,
                    'status': 'pending'
                })
                break
    
    # Remove duplicates
    action_items = deduplicate_items(action_items)
    
    logger.info(f"Extracted {len(action_items)} action items")
    return action_items

def extract_assignee(text, default_speaker):
    """Try to extract who is assigned an action item"""
    # Pattern: "John will...", "assign to Sarah", "Sarah should..."
    patterns = [
        r"assign(?:ed)?\s+to\s+([A-Z][a-z]+)",
        r"([A-Z][a-z]+)\s+(?:will|should|needs? to|must)",
        r"for\s+([A-Z][a-z]+)\s+to"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)
    
    return default_speaker

def extract_priority(text):
    """Extract priority level from text"""
    if re.search(r"\b(urgent|critical|ASAP|immediately|high priority)\b", text, re.IGNORECASE):
        return "HIGH"
    elif re.search(r"\b(low priority|when (you )?can|eventually)\b", text, re.IGNORECASE):
        return "LOW"
    else:
        return "MEDIUM"

def extract_deadline(text):
    """Extract deadline mentions"""
    deadline_patterns = [
        r"by\s+(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)",
        r"by\s+(tomorrow|today|tonight|this week|next week|end of (?:week|month))",
        r"due\s+(.+?)(?:\.|,|$)",
        r"deadline:?\s+(.+?)(?:\.|,|$)"
    ]
    
    for pattern in deadline_patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1)
    
    return None

def extract_key_points(transcript):
    """Extract key discussion points"""
    important_indicators = [
        "important", "key point", "main thing", "crucial", "essential",
        "remember", "note that", "keep in mind", "highlight", "emphasize",
        "priority", "focus on", "critical"
    ]
    
    key_points = []
    
    for segment in transcript:
        text = segment['text'].lower()
        
        # Check for importance indicators
        if any(indicator in text for indicator in important_indicators):
            key_points.append({
                'text': segment['text'],
                'speaker': segment['speaker'],
                'timestamp': segment['start'],
                'context': get_surrounding_context(transcript, segment)
            })
        
        # Also capture longer statements (likely important)
        elif len(segment['text'].split()) > 30:
            key_points.append({
                'text': segment['text'][:200] + '...' if len(segment['text']) > 200 else segment['text'],
                'speaker': segment['speaker'],
                'timestamp': segment['start'],
                'context': ''
            })
    
    # Limit to top 15 key points
    return key_points[:15]

def extract_decisions(transcript):
    """Extract decisions made during meeting"""
    decision_keywords = [
        "decide", "decision", "agreed", "agree", "consensus", "resolved",
        "concluded", "determined", "settled on", "going with", "chose",
        "approved", "accepted", "finalized"
    ]
    
    decisions = []
    
    for segment in transcript:
        text = segment['text'].lower()
        
        if any(keyword in text for keyword in decision_keywords):
            decisions.append({
                'text': segment['text'],
                'speaker': segment['speaker'],
                'timestamp': segment['start']
            })
    
    return deduplicate_items(decisions)

def extract_questions(transcript):
    """Extract unanswered questions"""
    questions = []
    
    for segment in transcript:
        text = segment['text']
        
        # Look for question marks or question words
        if '?' in text or re.search(r'\b(what|when|where|why|how|who|which)\b', text, re.IGNORECASE):
            questions.append({
                'text': text,
                'speaker': segment['speaker'],
                'timestamp': segment['start'],
                'answered': False  # Could implement answer detection
            })
    
    return questions[:10]  # Limit to 10 questions

def extract_topics(transcript):
    """Extract main topics discussed using simple keyword frequency"""
    from collections import Counter
    
    # Combine all text
    all_text = ' '.join([seg['text'] for seg in transcript])
    
    # Remove common words
    stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
                      'of', 'with', 'by', 'from', 'up', 'about', 'into', 'through', 'during',
                      'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
                      'do', 'does', 'did', 'will', 'would', 'should', 'could', 'may', 'might',
                      'i', 'you', 'he', 'she', 'it', 'we', 'they', 'them', 'their', 'this', 'that'])
    
    words = re.findall(r'\b[a-z]{4,}\b', all_text.lower())
    words = [w for w in words if w not in stop_words]
    
    # Count frequency
    word_freq = Counter(words)
    
    # Get top topics
    topics = [word for word, count in word_freq.most_common(15) if count > 2]
    
    return topics

def get_surrounding_context(transcript, target_segment, window=1):
    """Get context from surrounding segments"""
    idx = transcript.index(target_segment)
    start = max(0, idx - window)
    end = min(len(transcript), idx + window + 1)
    
    context = ' '.join([seg['text'] for seg in transcript[start:end] if seg != target_segment])
    return context[:100] + '...' if len(context) > 100 else context

def deduplicate_items(items):
    """Remove duplicate items based on text similarity"""
    if not items:
        return []
    
    unique_items = []
    seen_texts = set()
    
    for item in items:
        text_lower = item['text'].lower().strip()
        # Simple deduplication
        if text_lower not in seen_texts:
            seen_texts.add(text_lower)
            unique_items.append(item)
    
    return unique_items

def format_action_items(action_items):
    """Format action items as markdown list"""
    if not action_items:
        return "No action items identified."
    
    formatted = []
    for i, item in enumerate(action_items, 1):
        priority_emoji = "üî¥" if item['priority'] == "HIGH" else "üü°" if item['priority'] == "MEDIUM" else "üü¢"
        deadline_str = f" | Due: {item['deadline']}" if item['deadline'] else ""
        
        formatted.append(
            f"{i}. {priority_emoji} **[{item['timestamp']}] {item['speaker']}**: {item['text']}\n"
            f"   - *Assigned to:* {item['assigned_to']}{deadline_str}"
        )
    
    return '\n'.join(formatted)

def format_key_points(key_points):
    """Format key points as markdown list"""
    if not key_points:
        return "No key points identified."
    
    formatted = []
    for point in key_points:
        formatted.append(f"‚Ä¢ **[{point['timestamp']}] {point['speaker']}**: {point['text'][:150]}...")
    
    return '\n'.join(formatted)

def format_decisions(decisions):
    """Format decisions as markdown list"""
    if not decisions:
        return "No explicit decisions recorded."
    
    formatted = []
    for i, decision in enumerate(decisions, 1):
        formatted.append(f"{i}. **[{decision['timestamp']}] {decision['speaker']}**: {decision['text']}")
    
    return '\n'.join(formatted)

def format_questions(questions):
    """Format questions as markdown list"""
    if not questions:
        return "No open questions."
    
    formatted = []
    for q in questions:
        status = "‚úÖ Answered" if q.get('answered') else "‚ùì Open"
        formatted.append(f"‚Ä¢ {status} **[{q['timestamp']}] {q['speaker']}**: {q['text']}")
    
    return '\n'.join(formatted)

def format_topics(topics):
    """Format topics as comma-separated list"""
    if not topics:
        return "No topics identified."
    
    return ', '.join([f"**{topic}**" for topic in topics])

def generate_time_references(transcript):
    """Generate time-coded references to important moments"""
    if not transcript:
        return "No references available."
    
    references = []
    
    # Sample every ~5 minutes or key segments
    sample_interval = max(1, len(transcript) // 12)
    
    for i in range(0, len(transcript), sample_interval):
        segment = transcript[i]
        references.append(
            f"‚Ä¢ **{segment['start']}** - {segment['speaker']}: {segment['text'][:80]}..."
        )
    
    return '\n'.join(references[:15])

def calculate_duration(transcript):
    """Calculate total meeting duration"""
    if not transcript:
        return "0:00:00"
    
    total_seconds = transcript[-1]['end_seconds'] if 'end_seconds' in transcript[-1] else 0
    hours = int(total_seconds // 3600)
    minutes = int((total_seconds % 3600) // 60)
    seconds = int(total_seconds % 60)
    
    return f"{hours}:{minutes:02d}:{seconds:02d}"

def count_unique_speakers(transcript):
    """Count unique speakers in transcript"""
    speakers = set([seg['speaker'] for seg in transcript])
    return len(speakers)